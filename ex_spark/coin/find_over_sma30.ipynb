{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa019ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/21 20:35:04 WARN Utils: Your hostname, pire66-HP-Laptop-15-bw0xx resolves to a loopback address: 127.0.1.1; using 192.168.66.90 instead (on interface enp2s0)\n",
      "22/02/21 20:35:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pire66/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/21 20:35:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession \n",
    "sc = SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e76d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.66.90:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0291bae",
   "metadata": {},
   "source": [
    "на вход подается файл, ранее полученный используя api.coingecko.com:\n",
    "в нем ohlcv информация о монетах за 365 дней, \n",
    "Название используемого API: /coins/{id}/ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4601722",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "monet = sc.broadcast(\"aave\" )\n",
    "sma_size = sc.broadcast(30)\n",
    "days = sc.broadcast(365)\n",
    "namedir = monet.value + \"_for_\"+str(days.value) + \"_days\"\n",
    "filename = monet.value + \"_OHLC_\"+str(days.value ) + \".json\"\n",
    "filewithpath = namedir + \"/\" + filename\n",
    "schema_string = (\"time_coin long, open_coin double, high_coin double, low_coin double, close_coin double\")\n",
    "df = spark.read.csv(filewithpath,header = False,schema = schema_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce5fd8",
   "metadata": {},
   "source": [
    "По полученным данным посчитать 30ое скользящее стреднее(SMA30) \n",
    "и в отдельный файл вывести те ohlcv, в которых цена закрытия выше чем SMA30 в данном временном блоке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "547fd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+--------+----------+\n",
      "|    time_coin|open_coin|high_coin|low_coin|close_coin|\n",
      "+-------------+---------+---------+--------+----------+\n",
      "|1614470400000|   337.97|   392.58|  337.97|    392.58|\n",
      "|1615075200000|   397.86|   397.86|  369.65|     390.0|\n",
      "|1615420800000|   413.33|   446.27|  408.24|    408.24|\n",
      "|1615766400000|   404.96|   415.62|  373.83|    386.19|\n",
      "|1617148800000|   350.46|   376.98|  344.03|    376.98|\n",
      "|1617408000000|   381.15|   418.77|  381.15|    418.77|\n",
      "|1617753600000|   379.45|   404.02|  379.45|    383.93|\n",
      "|1618444800000|   370.51|   428.39|  370.51|    428.39|\n",
      "|1618790400000|   467.73|   467.73|  381.58|    381.58|\n",
      "|1619481600000|    343.7|   404.22|  316.44|    404.22|\n",
      "|1619740800000|   437.37|   461.48|  437.37|    438.43|\n",
      "|1620000000000|   444.08|   503.03|  444.08|    483.34|\n",
      "|1620345600000|   516.13|   516.13|  448.65|    451.53|\n",
      "|1620691200000|   445.42|   465.44|  434.65|    434.65|\n",
      "|1621036800000|   458.95|   595.76|  458.95|    595.76|\n",
      "|1621382400000|   514.65|   631.26|  514.65|    631.26|\n",
      "|1622073600000|   296.54|   413.81|  296.54|    413.81|\n",
      "|1628294400000|   309.96|   373.68|  309.96|    370.16|\n",
      "|1628640000000|   384.82|   385.21|  360.36|    385.21|\n",
      "|1628985600000|   404.06|    426.8|  391.97|    414.43|\n",
      "+-------------+---------+---------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window \n",
    "from pyspark.sql import functions as func \n",
    "sma_size = sc.broadcast(30) \n",
    "window = Window.partitionBy('temppartition').orderBy('time_coin').rowsBetween(-sma_size.value,0) \n",
    "df_out = df.withColumn('temppartition',func.current_date()).withColumn('sma', func.mean('close_coin').over(window)) \n",
    "df2 = df_out.filter(df_out.close_coin > df_out.sma).drop('temppartition','sma')\n",
    "df2.show()\n",
    "df2.write.csv('coins_over_mean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305401c7",
   "metadata": {},
   "source": [
    "отобранные данные выгружаются в папку 'coins_over_mean.csv' в одноименный файл в формате csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
